
---------- APACHE SPARK  -----------------
Apache Spark is a parallel processing framework and unified analytics engine that supports in memory processing to boost the performance of bigData Analytics./
-- Apache Spark is a parallel processing framework and an open-source unified analytics engine for large-scale data processing.
Spark was built on the top of the Hadoop MapReduce. 
** The main feature of Spark is its in-memory cluster computing that increases the processing speed of an application.

----- What is apache spark Used for -----------------
Spark is used for many types of data processing.It supports ETL(Extract,Transform and Load), interactive Queries(SQL), advanced analytics(Eg:Machine Learning) 
and structered streaming over large datasets.
-- Spark integrates with many storage systems(Eg: HDFS,Cassandra,MYSQL,HBase,MongoDB ,S3). 
-- Spark is also pluggable, with dozens of applications,data sources and environments.

----------- Features of Spark ---------------------
 Speed - Apache Spark is designed to be faster than other data processing systems, such as Hadoop. It can process data in memory, which makes it much faster than systems
 Ease of use
 Fault Tolerance - Apache Spark is designed to be fault-tolerant. Failures can be recovered from, and they can carry on uninterruptedly processing data.
 Scalability - Apache Spark is highly scalable and can handle large amounts of data.
 Compatibility - Apache Spark is compatible with a wide range of programming languages, including Java, Python, Scala, and R.
 Generality - It provides a collection of libraries including SQL and DataFrames, MLlib for machine learning, GraphX, and Spark Streaming.
 High-speed processing - Apache Spark is designed to process data in parallel, which means that it can process large volumes of data very quickly.
 
 ---------- Spark Architecture -------------------
 
The Spark follows the master-slave architecture. Its cluster consists of a single master and multiple slaves.
Driver Program -- The Driver Program is a process that runs the main() function of the application and creates the SparkContext object.
SparkContext -- The purpose of SparkContext is to coordinate the spark applications,which connects to the cluster manager.
Cluster Manager --The role of the cluster manager is to allocate resources across applications. The Spark is capable enough of running on a large number of clusters.
                  It consists of various types of cluster managers such as Hadoop YARN, Apache Mesos and Standalone Cluster Manager.
Worker Node -- The worker node is a slave node.Its role is to run the application code in the cluster.
The Spark executors -- Executors are worker processes responsible for executing tasks in Spark applications.
                        They are launched on worker nodes and communicate with the driver program and cluster manager. 
Master -- The machine on which the Driver Program runs. 
Slave -- The machine on which the Executor Program runs. 
Task --A unit of work that will be sent to one executor.


-------- sparkContext and sparkSession ------------
The purpose of SparkContext is to coordinate the spark applications,which connects to the cluster manager.
sparkContext was used as a channel to access all spark functionality.
**sparkConf is required to create the spark context object.

SparkSession: The Spark session object is the primary entry point for Spark applications, and allows you to run SQL queries on database tables.

Partitioning:  partitioning is a way to split the data into multiple partitions so that you can execute transformations on multiple partitions in parallel,
 which allows completing the job faster. 



-----------  spark Architecture Interview Answer ----------

The Spark architecture consists of several key components, including the Spark Driver, Spark Executors and Spark Cluster Manager.
 The Spark Driver coordinates and executes tasks. The Spark Executors are the worker nodes in the cluster, executing the tasks the Spark Driver has assigned to them.
The Spark Cluster Manager manages the resources in the cluster by allocating CPU, memory and network resources to the Spark Executors.
 Spark processes data by dividing it into partitions and distributing it amongst the Spark Executors.
 It also supports data caching and persistence, which allows for faster access to frequently used data and reduces the need for loading data from disk repeatedly.
 
 -----Components of Spark -------
 Spark Core -- The Spark Core is the heart of Spark and performs the core functionality.
            It holds the components for task scheduling, fault recovery, interacting with storage systems and memory management.
 Spark SQL -- The Spark SQL is built on the top of Spark Core. It provides support for structured data.
              It also supports various sources of data like Hive tables, Parquet, and JSON.
 Spark Streaming -- Spark Streaming is a Spark component that supports scalable and fault-tolerant processing of streaming data.
It uses Spark Core's fast scheduling capability to perform streaming analytics.
** It accepts data in mini-batches and performs RDD transformations on that data.
 MLlib -- The MLlib is a Machine Learning library that contains various machine learning algorithms.
 GraphX -- The GraphX is a library that is used to manipulate graphs and perform graph-parallel computations.
 
 ----------- Spark RDD --------------------

 RDD is a resilient distributed dataset. It is a collection of elements, partitioned across the nodes of the cluster so that we can execute various parallel operations on it.
It is an immutable distributed collection of objects.

There are two ways to create RDDs:

Parallelizing an existing data in the driver program.
Using the External Datsets i.e.. such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat.

******* •	Lazy Evaluation: Spark transformations done using Spark RDDs are lazy. 
Meaning, they do not generate results right away, but they create new RDDs from existing RDD. This lazy evaluation increases the system efficiency.
Parallelized Collections:
To create parallelized collection, call SparkContext's parallelize method on an existing collection in the driver program.
 Each element of collection is copied to form a distributed dataset that can be operated on in parallel.
 Ex: val info = Array(1, 2, 3, 4)  
val distinfo = sc.parallelize(info) 
 
 External Datasets:
SparkContext's textFile method can be used to create RDD's text file. EX: val data=sc.textFile("url")

--------- operations of RDD ---------
RDDs support two types of operations: 1) Transformations 2) Actions
Transformations:  Spark Transformation is a function that produces new RDD from the existing RDDs. It takes RDD as input and produces one or more RDD as output.

Transformations are of 2 types: 1) Narrow Transformations 2) Wide Transformations
Narrow Transformations: In Narrow transformation, all the elements that are required to compute the records in single partition live in the single partition of parent RDD.
i.e:  one parent ==> one child
EX:Map(),flatMap(),filter etc


Wide Transformations: In Wide transformation, all the elements that are required to compute the records in the single partition may live in many partitions of parent RDD.
i.e: one parent ==> Multiple Childs.
EX:reduceByKey(),groupByKey()


Transformations:
map( ) -- The map function iterates over every line in RDD and split into new RDD.
flatMap() -- A flatMap is a transformation operation. It applies to each element of RDD and it returns the result as new RDD.

***  Map and flatMap are similar in the way that they take a line from input RDD and apply a function on that line.
**  The key difference between map() and flatmap() is map() returns only one element, while flatMap() can return a list of elements.

Filter() -- Spark RDD filter() function returns a new RDD, containing only the elements that meet a predicate/condition is true.(what we are giving).
distinct() -- In Spark, the Distinct function returns the distinct elements from the provided dataset.
Union() -- In Spark, Union function returns a new dataset that contains the combination of elements present in the different datasets. 
EX: val a=List(1,2,3)  val b=List(4,5,6)  o/p: List(1,2,3,4,5,6)
Intersection() -- In Spark, Intersection function returns a new dataset that contains the intersection of elements present in the different datasets.So, it returns only a single row.
EX: val a=List(1,2,3) val b=List(3,4,5) o/p: List(3) i.e .. return common elements from the both the variables

SortByKey() -- In Spark, the sortByKey function maintains the order of elements.
 It receives key-value pairs (K, V) as an input, sorts the elements in ascending or descending order and generates a dataset in an order.
 Ex: val data = sc.parallelize(Seq(("C",3),("A",1),("D",4),("B",2),("E",5)))   //data is not in order
 val sortfunc = data.sortByKey()  o/p: Array((A,1),(B,2),(C,3),(D,4),(E,5))
 
groupByKey() --In Spark, the groupByKey function is a frequently used transformation operation that performs shuffling of data. 
It receives key-value pairs (K, V) as an input, group the values based on key and generates a dataset of (K, Iterable) pairs as an output.
     
reduceByKey() --  In Spark, the reduceByKey function is a frequently used transformation operation that performs aggregation of data. 
It receives key-value pairs (K, V) as an input, aggregates the values based on the key and generates a dataset of (K, V) pairs as an output.

Example: # Sample key-value pair RDD
data = [(“A”, 1), (“B”, 2), (“A”, 3), (“B”, 4), (“C”, 5)]

# Using groupByKey
grouped = rdd.groupByKey()
# Result: [(‘A’, [1, 3]), (‘B’, [2, 4]), (‘C’, [5])]


# Using reduceByKey
reduced = rdd.reduceByKey(lambda x, y: x + y)
# Result: [(‘A’, 4), (‘B’, 6), (‘C’, 5)]



*** Repartition : The repartition() can be used to increase or decrease the number of partitions, but it involves heavy data shuffling across the cluster.
*** Coalesce:  On the other hand, coalesce() can be used only to decrease the number of partitions.
To avoid full shuffling of data we use coalesce() function.  

Cache and Persist:
Spark Cache and persist are optimization techniques for iterative and interactive Spark applications to improve the performance of the jobs or applications.
Both caching and persisting are used to save the Spark RDD, Dataframe, and Dataset’s.
-- But, the difference is, RDD cache() method default saves it to memory (MEMORY_ONLY).
    whereas persist() method is used to store it to the user-defined storage level.
	
Actions in Rdd: Actions: which return a value to the driver program after running a computation on the dataset.
Collect() : We can read the result generated by RDD by using the collect operation.
Count: The count action is used to get the total number of elements present in the particular RDD.
reduce( ):The reduce() function takes the two elements as input from the RDD and then produces the output of the single element of same type.

Features of RDD:
Lazy Evvaluation,Fault tolerance, in memory computation,immutability,partioning

------------------ DAG --------------------------
Directed Acyclic Graph(DAG) is used to represent flow of operations or computations in a job/task. 
DAG is a set of Vertices and Edges, where vertices represent the RDDs and the edges represents the Operations to be applied on RDD.
** The DAG allows Spark to break down a large-scale data processing job into smaller, independent tasks that can be executed in parallel, optimize the job execution,
   achieve fault tolerance, reuse intermediate results, and provide a visual representation of the logical execution plan.
   
Dag visualization:   stage:1-->(Read File),stage:2-->(Filter Rdd), stage:3--> (Map Rdd),stage:4-->(Reduce rdd) =================> OUTPUT

The DAG breaks the job down into a sequence of stages, where each stage represents a group of tasks that can be executed independently of each other. 
The tasks within each stage can be executed in parallel across the machines.

DAG Scheduler:
In Spark, the DAG Scheduler is responsible for transforming a sequence of RDD transformations and actions into a directed acyclic graph (DAG) of stages and tasks,
 which can be executed in parallel across a cluster of machines. 
components of DAG:
Stages: A stage represents a set of tasks that can be executed in parallel.
Tasks: A task represents a single unit of work that can be executed on a single partition of an RDD. 
Dependencies: The dependencies between RDDs determine the order in which tasks are executed

---LINEAGE Graph---------------------
A lineage graph is a DAG(Directed acyclic Graph) and it is a visual representation of the logical execution plan for a Spark computation.
It shows the sequence of transformations applied to Resilient Distributed Datasets (RDDs) and the dependencies between them. 

Ex: RDD1 --> RDD2 --> RDD3

In the above flow, it's pretty clear that RDD2 gets created by RDD1 and RDD3 gets created by RDD2. 
The lineage graph represents the sequence of transformations applied to the original input data to produce a new RDD or DataFrame/Dataset.

---------------- another interview questions --------------------

-----------  Deploy modes in spark--------------------
There are 2 deploy modes in Spark. They are:

Client Mode: The deploy mode is said to be in client mode when the spark driver component runs on the machine node from where the spark job is submitted.
The main disadvantage of this mode is if the machine node fails, then the entire job fails.
Cluster Mode: If the spark job driver component does not run on the machine from which the spark job has been submitted, then the deploy mode is said to be in cluster mode.

---- Data formats supported by spark are: 
 File formats like paraquet, JSON, XML, CSV, RC, Avro, TSV, etc are supported by Spark
 
 --Shuffling: The Spark SQL shuffle is a mechanism for redistributing the data or re-partitioning data so that the data is grouped differently across partitions. 
 
 














